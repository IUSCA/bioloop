1. parallelism in long-running parallel operations - make sure parallel operations don't conflict one anohter
2. previously failed workflows - what happens?
3. some of renamed hardlinked dirs already present (from an older run of the script providing the same dir as arg)
4. dataset registration api call fails because dataset with this name and type already exists
5. for datasets that are already regisyered/registering, add log message. don't register them again.
6. cleanup any artifacts generated in the end. any renamed dirs should only be cleaed up if they have the ARCHIVE state among their corresponding datasets' states. 
7. if any datasets are not registered due to mismatch checkdums b/w src and hardlinks, log that it won't be registered
8. if registering subdirs, log any files inside provided dir that are not directories and therefore won't be registered (only first ten, followed by ... (add message saying 'showing first 10 files')).
10. dont register in case of checksum mismatch
11. make sure the script doesn't exit before all renamed subdirs (if any) and any other artifacts generated have been cleaned up.
12. if a dataset for an eligible candidate dir (renamed or original depending on use case) exists in the DB, but the Integrated workflow has NOT been kicked off on it, kick it off.
13. create single data dir of 500 mb for testing. verify that hardlink creation takes little to no time.
14. idempotence - if hardlinks exist from previous runs, they are deleted and recreated.
15. if the renamed dir parent dir cannot be created at the sibling level of the provided dir, log this and early fail.
16. link to Project
17. dry-run mode should not start actual workflows - verify that existing datasets without workflows are only simulated, not actually started
